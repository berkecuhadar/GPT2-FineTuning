{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NFyM-hMpkkxylTtSM3CYZJ4NSbMZzm_c","timestamp":1715958840813}],"collapsed_sections":["QODsweFJpwoD","tfLafCmtpxUZ","vayY0gBUEW_u","xMr6Ge5gon-g","6vrAbcvX8QXJ"],"machine_shape":"hm","mount_file_id":"1Ospz58QGoB-ETh79MALurwIOsbO9Ylq6","authorship_tag":"ABX9TyNz554tKsJTUigemawg3g1q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Preprocessing and Prepare data to single format**"],"metadata":{"id":"QODsweFJpwoD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8deSehioyc2"},"outputs":[],"source":["import pandas as pd\n","import pyarrow.parquet as pq\n","import json"]},{"cell_type":"code","source":["# parquet file to dataFrame\n","def par2Df(parquet_file):\n","    table = pq.read_table(parquet_file)\n","    df = table.to_pandas()\n","    drop_columns = [\"Biography\",\"Emotion\",\"Name\"]\n","    df.drop(drop_columns, axis=1, inplace=True)\n","    return df"],"metadata":{"id":"zCnKhs2u01Xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# json lines files to  dataframe\n","\n","def json2Df(json_file):\n","    datas = []\n","    dataList = []\n","\n","    # add only specific key in the list\n","    with open(json_file, \"r\") as file:\n","        for line in file:\n","            data = json.loads(line)\n","            datas.append(data[\"dialogue_text\"])\n","\n","    # remove unwanted string\n","    for element in datas:\n","      element = element.replace(\"Summarize the dialogue\",\"\")\n","      dataList.append(element)\n","\n","    datas = []\n","\n","    # split dialogues\n","    for element in dataList:\n","      element = str(element).strip().split(\"\\n\")\n","      datas.append(element)\n","\n","    # remove character's name\n","    for i in range(len(datas)):\n","      for j in range(len(datas[i])):\n","          last = datas[i][j].rfind(\":\")\n","          if datas[i][j].count(\":\") % 2 != 0:\n","              datas[i][j] = datas[i][j][last:]\n","              datas[i][j] = datas[i][j].replace(\":\",\"\")\n","\n","    # remove unanswered content\n","    for sublist in datas:\n","      if len(sublist) % 2 != 0:\n","        sublist.pop(-1)\n","\n","    # split by query and response\n","    query = []\n","    response = []\n","    for sublist in datas:\n","      for i in range(len(sublist)):\n","        if i % 2 == 0:\n","          query.append(sublist[i])\n","        else:\n","          response.append(sublist[i])\n","\n","    df = pd.DataFrame({'Query': query, 'Response': response})\n","    return df"],"metadata":{"id":"8y7I2-tn05Do"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["par_files = [\"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/raw_data/npc-dialogue/test-00000-of-00001-0408c6b1dfcc3c77.parquet\",\n","             \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/raw_data/npc-dialogue/train-00000-of-00001-4eeea4877d4ce970.parquet\"]\n","jsonl_files = [\"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/raw_data/light-batch-summarize-dialogue/test.jsonl\",\n","               \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/raw_data/light-batch-summarize-dialogue/train.jsonl\",\n","               \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/raw_data/light-batch-summarize-dialogue/valid.jsonl\"]"],"metadata":{"id":"97pgFozn1fqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create df for single format\n","par0 = par2Df(par_files[0])\n","par1 = par2Df(par_files[1])\n","\n","json0 = json2Df(jsonl_files[0])\n","json1 = json2Df(jsonl_files[1])\n","json2 = json2Df(jsonl_files[2])\n","\n","dataFrame = pd.concat([par0, par1, json0, json1, json2], ignore_index=True)\n","dataFrame.info()"],"metadata":{"id":"VTpGwokwgVo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# take samples for small dataset\n","smallDF = dataFrame.sample(100,random_state=50)\n","# take samples for medium dataset\n","mediumDF = dataFrame.sample(1000,random_state=50)\n","# use all data for large dataset\n","largeDF = dataFrame\n","\n","# save datasets  csv format\n","smallDF.to_csv(\"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/small_dataset.csv\", index=False)\n","mediumDF.to_csv(\"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/medium_dataset.csv\", index=False)\n","largeDF.to_csv(\"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/large_dataset.csv\", index=False)\n"],"metadata":{"id":"hZ8MWTN_hEIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Training & Save Models**"],"metadata":{"id":"tfLafCmtpxUZ"}},{"cell_type":"markdown","source":["### **Creating Training Model**"],"metadata":{"id":"vayY0gBUEW_u"}},{"cell_type":"code","source":["# use transformers and accelerate\n","!pip install transformers\n","!pip install accelerate"],"metadata":{"id":"ozUmGMmop15B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling"],"metadata":{"id":"Gkv3h8Yn6a2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_and_prepare_data(file_path):\n","    df = pd.read_csv(file_path)\n","    combined_texts = [\"Query: \" + str(row['Query']).lower() + \"\\nAnswer: \" + str(row['Response']).lower() for _, row in df.iterrows()]\n","    return combined_texts\n","\n","def tokenize_data(tokenizer, texts, block_size=128):\n","    tokenized_data = []\n","    for text in texts:\n","        tokenized_text = tokenizer(text, truncation=True, max_length=block_size, padding='max_length', return_tensors=\"pt\")\n","        tokenized_data.append(tokenized_text['input_ids'].squeeze())\n","    return tokenized_data\n","\n","def create_dataloader(tokenized_data, batch_size):\n","    dataset = torch.stack(tokenized_data)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader\n","\n","def train(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps):\n","    combined_texts = load_and_prepare_data(train_file_path)\n","\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenized_data = tokenize_data(tokenizer, combined_texts)\n","\n","    train_dataloader = create_dataloader(tokenized_data, per_device_train_batch_size)\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","    tokenizer.save_pretrained(output_dir)\n","\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    model.save_pretrained(output_dir)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        overwrite_output_dir=overwrite_output_dir,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        num_train_epochs=num_train_epochs,\n","        save_steps=save_steps,\n","        logging_steps=save_steps,\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataloader.dataset,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()"],"metadata":{"id":"qJR85k0f6fBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Train Test Model**"],"metadata":{"id":"tdQSkzk4pAWQ"}},{"cell_type":"code","source":["# args for training\n","train_file_path = \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/small_dataset.csv\" # must be dataset path\n","model_name = 'gpt2' #based model name\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/models/testModel'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 10\n","save_steps = 100"],"metadata":{"id":"ZuvTtpBNpAWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"],"metadata":{"id":"qd7lI0VRpAWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Small Data High Epoch Model**"],"metadata":{"id":"Qw1jDCxyLeOl"}},{"cell_type":"code","source":["train_file_path = \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/small_dataset.csv\"\n","model_name = 'gpt2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/models/smallModel'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 50\n","save_steps = 100"],"metadata":{"id":"WDWpL1XS60Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"],"metadata":{"id":"hxdInYMb6-Yx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Medium Data Medium Epoch Model**"],"metadata":{"id":"LFMY9S0HoV__"}},{"cell_type":"code","source":["train_file_path = \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/medium_dataset.csv\"\n","model_name = 'gpt2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/models/mediumModel'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 10\n","save_steps = 100"],"metadata":{"id":"lQp3qTg1oV__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"],"metadata":{"id":"JUGCDE7-oWAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Large Data Low Epoch**"],"metadata":{"id":"xMr6Ge5gon-g"}},{"cell_type":"code","source":["train_file_path = \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/data/processed_data/large_dataset.csv\"\n","model_name = 'gpt2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/models/largeModel'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 5\n","save_steps = 500"],"metadata":{"id":"D4qp2-uAon-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"],"metadata":{"id":"yUi6J4Ilon-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference for Testing Model**"],"metadata":{"id":"6vrAbcvX8QXJ"}},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","model_name = \"/content/drive/MyDrive/Colab Notebooks/graduation_project/preprocessing&training/models/mediumModel\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n"],"metadata":{"id":"8R0oB52n7Az_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = input()\n","query = f\"Query: {user_input} [END]\"\n","input_ids = tokenizer.encode(query, return_tensors=\"pt\",truncation=True)\n","output = model.generate(input_ids, max_length=50+len(user_input), num_return_sequences=1, temperature=0.7, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n","response = tokenizer.decode(output[0], skip_special_tokens=True)\n","response = response.replace(query, \"\").strip()\n","response = response.replace(\"Answer: \", \"\").strip()\n","print(response)\n"],"metadata":{"id":"4yFUP15Qyg5S"},"execution_count":null,"outputs":[]}]}